interface UsageTokens {
  prompt_tokens: number;
  completion_tokens: number;
  total_tokens: number;
}

interface ChatFormat {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface Responses {
  /**
   * The text edit is an older API, and more expensive.
   */
  'v1/edits': {
    request: {
      // ID of the model to use
      model: 'text-davinci-edit-001' | 'code-davinci-edit-001';

      // The input text to use as a starting point for the edit.
      input?: '';

      // The instruction that tells the model how to edit the prompt.
      instruction: string;

      // How many edits to generate for the input and instruction.
      n?: number;

      // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
      // make the output more random, while lower values like 0.2 will make it more
      // focused and deterministic.
      //
      // We generally recommend altering this or top_p but not both.
      temperature: number;

      // An alternative to sampling with temperature, called nucleus sampling, where the
      // model considers the results of the tokens with top_p probability mass. So 0.1
      // means only the tokens comprising the top 10% probability mass are considered.

      // We generally recommend altering this or temperature but not both.
      top_p: number;
    };
    response: {
      object: 'edit';
      created: number;
      choices: Array<{
        // The edited text.
        text: string;
        index: 0;
      }>;
      usage: UsageTokens;
    };
  };
  /**
   * This is what is running ChatGPT.
   *
   * https://platform.openai.com/docs/api-reference/chat/create
   */
  'v1/chat/completions': {
    request: {
      // ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported.
      // https://platform.openai.com/docs/api-reference/chat/create#chat/create-model
      model: 'gpt-3.5-turbo' | 'gpt-3.5-turbo-0301';

      // The messages to generate chat completions for, in the chat format.
      messages: ChatFormat[];

      // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
      // make the output more random, while lower values like 0.2 will make it more
      // focused and deterministic.
      //
      // We generally recommend altering this or top_p but not both.
      //
      // Defaults to 1
      temperature?: number;

      // An alternative to sampling with temperature, called nucleus sampling, where
      // the model considers the results of the tokens with top_p probability mass.
      // So 0.1 means only the tokens comprising the top 10% probability mass
      // are considered.
      //
      // We generally recommend altering this or temperature but not both.
      //
      // Defaults to 1
      top_p?: number;

      // How many chat completion choices to generate for each input message.
      // Defaults to 1.
      n?: 1;

      // If set, partial message deltas will be sent, like in ChatGPT. Tokens will be
      // sent as data-only server-sent events as they become available, with the stream
      // terminated by a data: [DONE] message.
      // https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format
      stream?: boolean;

      // Up to 4 sequences where the API will stop generating further tokens.
      stop?: string[] | string;

      // The maximum number of tokens allowed for the generated answer. By default,
      // the number of tokens the model can return will be (4096 - prompt tokens).
      max_tokens?: number;

      // Number between -2.0 and 2.0. Positive values penalize new tokens based on
      // whether they appear in the text so far, increasing the model's likelihood
      // to talk about new topics.
      presence_penalty?: number;

      // Number between -2.0 and 2.0. Positive values penalize new tokens based on their
      // existing frequency in the text so far, decreasing the model's likelihood to
      // repeat the same line verbatim.
      frequency_penalty?: number;

      // Accepts a json object that maps tokens (specified by their token ID in the
      // tokenizer) to an associated bias value from -100 to 100. Mathematically, the
      // bias is added to the logits generated by the model prior to sampling. The
      // exact effect will vary per model, but values between -1 and 1 should decrease
      // or increase likelihood of selection; values like -100 or 100 should result in
      // a ban or exclusive selection of the relevant token.
      logit_bias?: string;

      // A unique identifier representing your end-user, which can help OpenAI to monitor
      // and detect abuse.
      user?: string;
    };
    response: {
      // e.g. 'chatcmpl-abc123'
      id: string;
      object: 'chat.completion';
      // Timestamp
      created: number;
      model: 'gpt-3.5-turbo-0301';
      usage: UsageTokens;
      choices: Array<{
        message: ChatFormat;
        finish_reason: 'stop';
        index: 0;
      }>;
    };
  };
}

export class OpenAI {
  key: string;

  constructor(key: string) {
    this.key = key;
  }

  async #send<EndPoint extends keyof Responses>(
    endPoint: EndPoint,
    data: Responses[EndPoint]['request'],
  ): Promise<Responses[EndPoint]['response']> {
    const response = await fetch('https://api.openai.com/' + endPoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${this.key}`,
      },
      body: JSON.stringify(data),
    });
    return response.json();
  }

  async runTest() {
    try {
      const data = await this.#send('v1/chat/completions', {
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: 'Say this is a test!' }],
        temperature: 0.7,
      });
      console.log('Data response', data);
    } catch (error) {
      console.error(error);
    }
  }

  async suggestTranslationCorrection(
    fromLanguage: string,
    toLanguage: string,
    fromText: string,
    toText: string,
  ) {
    function getUserPrompt(fromText: string, toText: string) {
      return [
        'Suggest improvements for the translation and list out the changes.',
        'From:',
        fromText,
        '',
        'To:',
        toText,
      ].join('\n');
    }

    return this.#send('v1/chat/completions', {
      model: 'gpt-3.5-turbo',
      messages: [
        {
          role: 'system',
          content: `You are a helpful assistant that corrects translations ${fromLanguage} to ${toLanguage}.`,
        },
        {
          role: 'user',
          content: getUserPrompt(
            'This dog is cute.',
            'Esta perro esta bonito.',
          ),
        },
        {
          role: 'assistant',
          content: [
            `1. "Esta" is a feminine pronoun which is incorrect for "perro" (a masculine noun). It should be "Este" instead.`,
            `2. "Bonito" is not incorrect, but a more common and appropriate word for "cute" in Spanish is "lindo".`,
            '',
            'Este perro es lindo.',
          ].join('\n'),
        },
        {
          role: 'user',
          content: getUserPrompt('This dog is cute.', 'Este perro es lindo.'),
        },
        {
          role: 'assistant',
          content: ['The translation appears good to me!'].join(''),
        },
        {
          role: 'user',
          content: getUserPrompt(fromText, toText),
        },
      ],
    });
  }
}
